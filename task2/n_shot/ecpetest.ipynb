{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from container import Container\n",
    "from template import Template, Example\n",
    "api_key = 'OpenAI-Key'\n",
    "model = Container(api_key, 'text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/nbhxgzzj2ds2h3yjxjyn8f2w0000gn/T/ipykernel_13306/1599189418.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  five_shot_train = train.loc[train['pairs'].apply(len) == 1].sample(3).append(\n",
      "/var/folders/sb/nbhxgzzj2ds2h3yjxjyn8f2w0000gn/T/ipykernel_13306/1599189418.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  five_shot_train = train.loc[train['pairs'].apply(len) == 1].sample(3).append(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dataload import parse_data\n",
    "train = pd.read_json('train.json')\n",
    "test = pd.read_json('example.json')\n",
    "one_shot_train = train.loc[train['pairs'].apply(len) == 1].sample(1)\n",
    "five_shot_train = train.loc[train['pairs'].apply(len) == 1].sample(3).append(\n",
    "    train.loc[train['pairs'].apply(len) == 2].sample(1)).append(\n",
    "    train.loc[train['pairs'].apply(len) == 3].sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_train_1 = parse_data(one_shot_train)\n",
    "parse_train_5 = parse_data(five_shot_train)\n",
    "# parse_train_6 = parse_data(ten_shot_train)\n",
    "parse_test = parse_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_temp = '''Extract emotion-cause pairs and return a list of tuple based on the given content.\n",
    "In each tuple, the first element is the index of the emotion clause and the second element is the index of cause clause.'''\n",
    "\n",
    "question_temp_pair_num_limit = '''Extract 1 to 4 emotion-cause pairs and return a list of tuple based on the given content.\n",
    "In each tuple, the first element is the index of the emotion clause and the second element is the index of cause clause.'''\n",
    "\n",
    "question_temp_sequence_prompt = '''Extract emotion-cause pairs and return a list of tuple based on the given content.\n",
    "In each tuple, the first element is the index of the emotion clause and the second element is the index of cause clause.\n",
    "The cause clause usually appears in the front of emotion clause'''\n",
    "\n",
    "question_temp_both_hint = '''Extract 1 to 4 emotion-cause pairs and return a list of tuple based on the given content.\n",
    "In each tuple, the first element is the index of the emotion clause and the second element is the index of cause clause.\n",
    "The cause clause usually appears in the front of emotion clause'''\n",
    "\n",
    "DATA = parse_train_5\n",
    "FILENAME = 'prompt_5_shot.txt'\n",
    "RESULTNAME = 'result_5_shot.txt'\n",
    "QUESTION = question_temp_both_hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = [{'content': e['content'], 'pair': e['pair']} for e in DATA]\n",
    "dev = [{'content': e['content'], 'pair': e['pair']} for e in parse_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Question:\\n', '{$ Question needs answer}'],\n",
       " ['Examples:\\n',\n",
       "  '[{$ Several representative examples of the given question with answers}]'],\n",
       " ['Content:\\n',\n",
       "  '{$ Passage with index which includes the emotion-cause pairs for Task 4}']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Think step by step. Give a response to the question in this format\"\n",
    "question = ['Question:\\n', '{$ Question needs answer}']\n",
    "train = ['Examples:\\n', '[{$ Several representative examples of the given question with answers}]']\n",
    "content = ['Content:\\n', '{$ Passage with index which includes the emotion-cause pairs for Task 4}']\n",
    "\n",
    "template = Template(instruction=instruction)\n",
    "template(question=question, train=train, content=content)\n",
    "# template(question=question, content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_ecpe(model, template, content, temp=0.7, n=2, question=QUESTION):\n",
    "    train = few_shot[:10]\n",
    "    example = Example(question=question, train=train, content=content)\n",
    "    completion, prompt = model.chat(template, example, temp, n)\n",
    "    return completion, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:52<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "prompts = []\n",
    "predict = []\n",
    "result = []\n",
    "for i in tqdm(range(100)):\n",
    "    completion, prompt = few_shot_ecpe(model, template, dev[i]['content'], temp=0.3, n=1)\n",
    "    predict.append(completion.choices[0].text)\n",
    "    prompts.append(prompt)\n",
    "    result.append(dev[i]['pair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_list(data):\n",
    "    result = []\n",
    "    for i in range(len(data)):\n",
    "        predicted = data[i].replace('(', '[').replace(')', ']') #replace all () to []\n",
    "        try:\n",
    "            predicted = predicted.split(':')[1]\n",
    "        except:\n",
    "            pass\n",
    "        predicted = predicted.strip() # strip all begining space\n",
    "        predicted = eval(predicted)\n",
    "        result.append(predicted)\n",
    "    return result\n",
    "\n",
    "predict = parse_to_list(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0954356846473029, 0.2, 0.12921348314606743, 23, 241, 115)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare(Y1, Y2):\n",
    "    '''Y1: Y_hat case, Y2: Y case'''\n",
    "    correct = 0\n",
    "    for c in Y2:\n",
    "        if c in Y1:\n",
    "            correct += 1\n",
    "    return correct, len(Y1), len(Y2)\n",
    "\n",
    "def calc_precision(Y_hat, Y):\n",
    "    assert len(Y_hat) == len(Y), print('shape mismatched')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ground_true = 0\n",
    "    for i in range(len(Y_hat)):\n",
    "        c, t, a = compare(Y_hat[i], Y[i])\n",
    "        correct += c\n",
    "        total += t\n",
    "        ground_true += a\n",
    "    precision = correct/total\n",
    "    recall = correct/ground_true\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1, correct, total, ground_true\n",
    "\n",
    "calc_precision(predict, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILENAME, 'w') as f:\n",
    "    for sentence in prompts:\n",
    "        f.write(sentence + '---------------End Case---------------\\n\\n')\n",
    "\n",
    "with open(RESULTNAME, 'w') as f:\n",
    "    for index in range(len(predict)):\n",
    "        res = predict[index]\n",
    "        re = result[index]\n",
    "        f.write(str(res) + '---------------True---------------' + str(re) + '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
